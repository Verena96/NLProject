{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libs necessárias\n",
    "import utils\n",
    "from utils import *\n",
    "import spacy\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from time import time \n",
    "import re\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from calendar import month_name\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5015, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>positivity</th>\n",
       "      <th>positivity:confidence</th>\n",
       "      <th>relevance</th>\n",
       "      <th>relevance:confidence</th>\n",
       "      <th>orig__golden</th>\n",
       "      <th>articleid</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>lineid</th>\n",
       "      <th>next_sentence</th>\n",
       "      <th>positivity_gold</th>\n",
       "      <th>previous_sentence</th>\n",
       "      <th>relevance_gold</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>729487630</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>6/8/15 14:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>not sure</td>\n",
       "      <td>0.3469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109092213</td>\n",
       "      <td>2/23/93</td>\n",
       "      <td>Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...</td>\n",
       "      <td>109092213_01</td>\n",
       "      <td>The Nasdaq composite index, home of technology...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The stock market accelerated its screeching sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>729487631</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>6/11/15 10:58</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.3675</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109092213</td>\n",
       "      <td>2/23/93</td>\n",
       "      <td>Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...</td>\n",
       "      <td>109092213_02</td>\n",
       "      <td>The bond market continued to rally, propelling...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The stock market accelerated its screeching sw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Nasdaq composite index, home of technology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>729487632</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>6/6/15 0:15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.3416</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109092213</td>\n",
       "      <td>2/23/93</td>\n",
       "      <td>Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...</td>\n",
       "      <td>109092213_03</td>\n",
       "      <td>The Nasdaq market was stricken by the collapse...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Nasdaq composite index, home of technology...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The bond market continued to rally, propelling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>729487633</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>6/14/15 20:27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6756</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109092213</td>\n",
       "      <td>2/23/93</td>\n",
       "      <td>Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...</td>\n",
       "      <td>109092213_04</td>\n",
       "      <td>uring the marketÛªs split ÛÓ was 4.26 percen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The bond market continued to rally, propelling...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Nasdaq market was stricken by the collapse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>729487634</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>6/6/15 13:22</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6509</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109092213</td>\n",
       "      <td>2/23/93</td>\n",
       "      <td>Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...</td>\n",
       "      <td>109092213_05</td>\n",
       "      <td>. \"It's a nervous market,Û said Lawrence R. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Nasdaq market was stricken by the collapse...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uring the marketÛªs split ÛÓ was 4.26 percen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  729487630    False   finalized                   3      6/8/15 14:26   \n",
       "1  729487631    False   finalized                   3     6/11/15 10:58   \n",
       "2  729487632    False   finalized                   3       6/6/15 0:15   \n",
       "3  729487633    False   finalized                   3     6/14/15 20:27   \n",
       "4  729487634    False   finalized                   3      6/6/15 13:22   \n",
       "\n",
       "   positivity  positivity:confidence relevance  relevance:confidence  \\\n",
       "0         NaN                 0.0000  not sure                0.3469   \n",
       "1         6.0                 0.3675       yes                1.0000   \n",
       "2         5.0                 0.3416       yes                0.6771   \n",
       "3         4.0                 0.6756       yes                1.0000   \n",
       "4         3.0                 0.6509       yes                1.0000   \n",
       "\n",
       "  orig__golden  articleid     date  \\\n",
       "0          NaN  109092213  2/23/93   \n",
       "1          NaN  109092213  2/23/93   \n",
       "2          NaN  109092213  2/23/93   \n",
       "3          NaN  109092213  2/23/93   \n",
       "4          NaN  109092213  2/23/93   \n",
       "\n",
       "                                            headline        lineid  \\\n",
       "0  Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...  109092213_01   \n",
       "1  Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...  109092213_02   \n",
       "2  Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...  109092213_03   \n",
       "3  Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...  109092213_04   \n",
       "4  Nasdaq Index Falls 1.7% But Dow Stocks Are Up:...  109092213_05   \n",
       "\n",
       "                                       next_sentence positivity_gold  \\\n",
       "0  The Nasdaq composite index, home of technology...             NaN   \n",
       "1  The bond market continued to rally, propelling...             NaN   \n",
       "2  The Nasdaq market was stricken by the collapse...             NaN   \n",
       "3  uring the marketÛªs split ÛÓ was 4.26 percen...             NaN   \n",
       "4  . \"It's a nervous market,Û said Lawrence R. ...             NaN   \n",
       "\n",
       "                                   previous_sentence relevance_gold  \\\n",
       "0                                                NaN            NaN   \n",
       "1  The stock market accelerated its screeching sw...            NaN   \n",
       "2  The Nasdaq composite index, home of technology...            NaN   \n",
       "3  The bond market continued to rally, propelling...            NaN   \n",
       "4  The Nasdaq market was stricken by the collapse...            NaN   \n",
       "\n",
       "                                                text  \n",
       "0  The stock market accelerated its screeching sw...  \n",
       "1  The Nasdaq composite index, home of technology...  \n",
       "2  The bond market continued to rally, propelling...  \n",
       "3  The Nasdaq market was stricken by the collapse...  \n",
       "4  uring the marketÛªs split ÛÓ was 4.26 percen...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load csv file containing labeled economic news\n",
    "#df = pd.read_csv('Full-Economic-News-DFE-839861.csv',encoding='latin1')\n",
    "df = pd.read_csv('us-economic-newspaper.csv',encoding='latin1')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0    814\n",
       "6.0    548\n",
       "3.0    541\n",
       "5.0    364\n",
       "7.0    249\n",
       "1.0    155\n",
       "2.0    149\n",
       "8.0     65\n",
       "9.0     14\n",
       "Name: positivity, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking categories\n",
    "df['positivity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_unit_id                    0\n",
       "_golden                     0\n",
       "_unit_state                 0\n",
       "_trusted_judgments          0\n",
       "_last_judgment_at          40\n",
       "positivity               2116\n",
       "positivity:confidence     942\n",
       "relevance                   8\n",
       "relevance:confidence        8\n",
       "orig__golden             4998\n",
       "articleid                   0\n",
       "date                        0\n",
       "headline                    0\n",
       "lineid                      0\n",
       "next_sentence               2\n",
       "positivity_gold          4980\n",
       "previous_sentence        1002\n",
       "relevance_gold           4958\n",
       "text                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking NaN values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
       "       'positivity', 'articleid', 'date', 'headline', 'lineid', 'text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove empty columns\n",
    "df.drop(['positivity:confidence','positivity_gold','positivity_gold','relevance_gold','_last_judgment_at',\n",
    "         'relevance','relevance:confidence','orig__golden','next_sentence','previous_sentence'],axis=1,inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove empty columns\n",
    "#df.drop(['positivity:confidence','positivity_gold','positivity_gold','relevance_gold'],axis=1,inplace=True)\n",
    "#df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate categories into 0 or 1\n",
    "#0 is negative and 1 is positive - obtain more balanced dataset\n",
    "#lista_class = []\n",
    "\n",
    "#for i in df['positivity']:\n",
    "#    if i < 6:\n",
    "#        lista_class+=[0] #negative\n",
    "#    else:\n",
    "#        lista_class+=[1] #positive\n",
    "#df['class'] = pd.DataFrame(lista_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate categories into 0 or 1\n",
    "#0 is negative and 1 is positive - obtain more balanced dataset\n",
    "lista_class = []\n",
    "\n",
    "for i in df['positivity']:\n",
    "    if i < 5:\n",
    "        lista_class+=[0] #negative\n",
    "    else:\n",
    "        lista_class+=[1] #positive\n",
    "df['class'] = pd.DataFrame(lista_class) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3356\n",
       "0    1659\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking new classes\n",
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2899, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_unit_id              0\n",
       "_golden               0\n",
       "_unit_state           0\n",
       "_trusted_judgments    0\n",
       "positivity            0\n",
       "articleid             0\n",
       "date                  0\n",
       "headline              0\n",
       "lineid                0\n",
       "text                  0\n",
       "class                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove Nan\n",
    "df=df.dropna()\n",
    "print(df.shape)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning\n",
    "d = df.copy()\n",
    "d['text'] = d['text'].apply(lambda x:x.lower())\n",
    "d['text'] = d['text'].str.replace('</br>',' ')\n",
    "d['text'] = d['text'].str.replace('-',' ')\n",
    "\n",
    "#Remove ponctuation\n",
    "exclude = set(string.punctuation)\n",
    "d['text'] = d['text'].apply(lambda texto:''.join(ch for ch in texto if ch not in exclude))\n",
    "d['text'] = d['text'].apply(lambda x:clean_ents(x))\n",
    "d['text'] = d['text'].apply(lambda x:sub_str(x))\n",
    "d['text'] = d['text'].apply(lambda x:clean_words(x))\n",
    "d = d.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('market', 647),\n",
       " ('rates', 558),\n",
       " ('rate', 527),\n",
       " ('interest', 471),\n",
       " ('prices', 439),\n",
       " ('inflation', 410),\n",
       " ('economic', 388),\n",
       " ('economy', 380),\n",
       " ('money', 252),\n",
       " ('unemployment', 233),\n",
       " ('higher', 228),\n",
       " ('price', 226),\n",
       " ('increase', 222),\n",
       " ('average', 215),\n",
       " ('growth', 214),\n",
       " ('recession', 207),\n",
       " ('rose', 200),\n",
       " ('decline', 186),\n",
       " ('tax', 181),\n",
       " ('rise', 180),\n",
       " ('term', 174),\n",
       " ('index', 169),\n",
       " ('business', 162),\n",
       " ('high', 160),\n",
       " ('long', 158),\n",
       " ('credit', 158),\n",
       " ('markets', 152),\n",
       " ('fell', 146),\n",
       " ('industrial', 146),\n",
       " ('bonds', 145)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count most frequent words in dataset\n",
    "Counter(\" \".join(d[\"text\"]).split()).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da base de features: (2899, 2)\n",
      "Tamanho da base de labels: (2899,)\n"
     ]
    }
   ],
   "source": [
    "#Machine Learning Algo\n",
    "#Features para criação do modelo preditivo\n",
    "\n",
    "features = d[['articleid','text']]\n",
    "\n",
    "labels = d['class']\n",
    "\n",
    "print('Tamanho da base de features:',features.shape)\n",
    "\n",
    "print('Tamanho da base de labels:',labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pacotes para Vetorização dos Textos e Machine Learning (varios testes para modelos distintos)\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1942, 2)\n",
      "(1942,)\n"
     ]
    }
   ],
   "source": [
    "#Split em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,labels, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding more stopwords\n",
    "stopw += ['said','president','wall','street','standard','poors','ll','ve','vice','dow','jones','nasdaq','th',\n",
    "          'chairman', 'chief', 'executive','new','york','analysts','federal','investors','economist','government',\n",
    "          'economists','dollar','point','stock','time','stocks','points','bank','bond','nations','traders','face',\n",
    "          'times','yields','generally','data','meeting','committee','figures','director','chairman','going','home','copom',\n",
    "          'session','people','public','according','blue','chip','feds','banks','area','city','says','officials','day','secretary',\n",
    "          'report','showed','todays','health','middle','class','crude','oil','focus','survey','ex','ante']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58003120124805\n",
      "0.0018193297643823029\n"
     ]
    }
   ],
   "source": [
    "#Pipeline com aplicação do Modelo Preditivo - LogReg\n",
    "\n",
    "pipeline = Pipeline([\n",
    "\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stopw,strip_accents='unicode',ngram_range=(2,4),\n",
    "\n",
    "    max_features=300)),\n",
    "\n",
    "    ('clf',LogisticRegression(penalty='l1', C = 0.55))])\n",
    "\n",
    " \n",
    "\n",
    "#Média e desvio padrão do modelo usando cross-validation\n",
    "\n",
    "score_nb=cross_val_score(pipeline,X_train['text'],y_train,cv=StratifiedShuffleSplit(n_splits=5, test_size=0.33, random_state=42),scoring='f1_micro')\n",
    "\n",
    "print(score_nb.mean())\n",
    "\n",
    "print(score_nb.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5590386624869383\n"
     ]
    }
   ],
   "source": [
    "# Fit do pipeline\n",
    "\n",
    "pipeline.fit(X_train['text'], y_train)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(y_test, pipeline.predict(X_test['text']), average = 'micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1.091135</td>\n",
       "      <td>index rose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.636293</td>\n",
       "      <td>prime rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0.518250</td>\n",
       "      <td>total return</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.391397</td>\n",
       "      <td>industrial average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.381318</td>\n",
       "      <td>mutual fund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>-0.149100</td>\n",
       "      <td>rate inflation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.161947</td>\n",
       "      <td>bad news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.267513</td>\n",
       "      <td>credit card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-0.351935</td>\n",
       "      <td>index fell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-0.549839</td>\n",
       "      <td>food prices</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         coef                word\n",
       "111  1.091135          index rose\n",
       "201  0.636293          prime rate\n",
       "285  0.518250        total return\n",
       "112  0.391397  industrial average\n",
       "171  0.381318         mutual fund\n",
       "..        ...                 ...\n",
       "217 -0.149100      rate inflation\n",
       "9   -0.161947            bad news\n",
       "42  -0.267513         credit card\n",
       "110 -0.351935          index fell\n",
       "78  -0.549839         food prices\n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking LR coefs\n",
    "coefs = pd.DataFrame({\"coef\":pipeline['clf'].coef_[0],\"word\":pipeline['tfidf'].get_feature_names()}).sort_values(\"coef\", \n",
    "ascending=False)#.query(\"coef!=0\")\n",
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "#Adding words and phrases that are going to be relevant for BCB texts, not exactly for the database used to train the algo\n",
    "\n",
    "vocabulary={'growth slowdown':300,\n",
    "            'causing significant slowdown':301,\n",
    "         'remains challenging':302,\n",
    "         'sharp drop':303,\n",
    "         'historically low levels':304,\n",
    "         'supply shock':305,\n",
    "         'economic recovery delayed':306,\n",
    "         'remains uncertain': 307,\n",
    "         'economic contraction':308,\n",
    "         'uncertainty arises':309,\n",
    "         'favorably expected signs':310,\n",
    "         'consistent stabilization':311,\n",
    "         'gradual recovery':312,\n",
    "         'favorable global economic activity':313,\n",
    "         'consistent recovery':314,\n",
    "         'prospects economic recovery':315,\n",
    "         'anchored expectations':316,\n",
    "         'frustration expectations':317,\n",
    "         'evolved favorably':318,\n",
    "         'benign evolution':319,\n",
    "         'gradual normalization':320,\n",
    "         'levels comfortable':321,\n",
    "         'conditions favorable':322,\n",
    "         'easing process continue':323,\n",
    "         'employment gains':324, 'recovery advances':325, 'favorable evolution':326, 'consumption growth':327,\n",
    "         'stronger recovery':328,'evolved expected':329,'lower global growth':330,'benign behavior':331, 'comfortable levels':332\n",
    "    \n",
    "}\n",
    "\n",
    " \n",
    "#Creating new vocab for TFIDF\n",
    "d=pipeline['tfidf'].vocabulary_\n",
    "\n",
    "od = collections.OrderedDict(sorted(d.items(),reverse=False,key=lambda x:x[0]))\n",
    "\n",
    "n=len(od)\n",
    "\n",
    "for feature, index in vocabulary.items():\n",
    "\n",
    "    if feature not in od.values():\n",
    "\n",
    "        od[feature] = n + index\n",
    "\n",
    "#Final vocab\n",
    "\n",
    "final_vocab = {key: v for key,v in zip(od,range(0,len(od)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.091135</td>\n",
       "      <td>index rose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.636293</td>\n",
       "      <td>prime rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.518250</td>\n",
       "      <td>total return</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.391397</td>\n",
       "      <td>industrial average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.381318</td>\n",
       "      <td>mutual fund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>stronger recovery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>evolved expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>lower global growth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>benign behavior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>comfortable levels</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>332 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         coef                 word\n",
       "0    1.091135           index rose\n",
       "1    0.636293           prime rate\n",
       "2    0.518250         total return\n",
       "3    0.391397   industrial average\n",
       "4    0.381318          mutual fund\n",
       "..        ...                  ...\n",
       "327  2.000000    stronger recovery\n",
       "328  1.000000     evolved expected\n",
       "329 -1.000000  lower global growth\n",
       "330  1.000000      benign behavior\n",
       "331  1.000000   comfortable levels\n",
       "\n",
       "[332 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inputing random weights for the words added above\n",
    "dict_weights = {\n",
    "            'growth slowdown':-0.5,\n",
    "            'causing significant slowdown':-0.5,\n",
    "         'remains challenging':-0.2,\n",
    "         'sharp drop':-0.7,\n",
    "         'historically low levels':-1,\n",
    "         'supply shock':-0.5,\n",
    "         'economic recovery delayed':-0.5,\n",
    "         'remains uncertain': -0.5,\n",
    "         'economic contraction':-1,\n",
    "         'uncertainty arises':-0.5,\n",
    "         'favorably expected signs':2,\n",
    "         'consistent stabilization':1,\n",
    "         'gradual recovery':1,\n",
    "         'favorable global economic activity':1,\n",
    "         'consistent recovery':1,\n",
    "         'prospects economic recovery':1,\n",
    "         'anchored expectations':1,\n",
    "         'frustration expectations':-0.5,\n",
    "         'evolved favorably':2,\n",
    "         'benign evolution':2,\n",
    "         'gradual normalization':1.5,\n",
    "         'levels comfortable':2,\n",
    "         'conditions favorable':2,\n",
    "         'easing process continue':1,'employment gains':2, 'recovery advances':2, 'favorable evolution':1, 'consumption growth':2,\n",
    "         'stronger recovery':2,'evolved expected':1,'lower global growth':-1,'benign behavior':1, 'comfortable levels':1\n",
    "    \n",
    "}\n",
    "\n",
    "#Append weights to coefs from LR\n",
    "for x,y in zip(list(dict_weights.keys()),list(dict_weights.values())):\n",
    "    \n",
    "    if x in list(coefs['word']):\n",
    "        coefs.loc[coefs['word'] == x]['coef'].values[0] = y\n",
    "    \n",
    "    else:\n",
    "        coefs = coefs.append({'coef':y,'word':x},ignore_index=True)\n",
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5797191887675507\n",
      "0.003638659528764625\n"
     ]
    }
   ],
   "source": [
    "#Check pipeline again, so we can use it later on BCB texts\n",
    "pipeline_ = Pipeline([\n",
    "\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stopw,strip_accents='unicode',ngram_range=(2,4),\n",
    "\n",
    "    vocabulary = final_vocab)),\n",
    "\n",
    "    ('clf',LogisticRegression(penalty='l1', C = 0.65))])\n",
    "\n",
    "\n",
    "#New list with the coefs added above, with new weights included\n",
    "list_coefs = np.array([coefs['coef'].to_list()])\n",
    "\n",
    "\n",
    "score_nb=cross_val_score(pipeline_,X_train['text'],y_train,cv=StratifiedShuffleSplit(n_splits=5, test_size=0.33, random_state=42),scoring='f1_micro')\n",
    "\n",
    "print(score_nb.mean())\n",
    "\n",
    "print(score_nb.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5579937304075235\n"
     ]
    }
   ],
   "source": [
    "# Fit do pipeline\n",
    "\n",
    "pipeline_.fit(X_train['text'], y_train)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(y_test, pipeline_.predict(X_test['text']), average = 'micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5538140020898642\n"
     ]
    }
   ],
   "source": [
    "#Test with new coefs\n",
    "\n",
    "#Setting new coefficients\n",
    "pipeline_['clf'].coef_ = list_coefs\n",
    "\n",
    "print(f1_score(y_test, pipeline_.predict(X_test['text']), average = 'micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>conditions favorable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>favorably expected signs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>evolved favorably</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>benign evolution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>stronger recovery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>consumption growth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>recovery advances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>employment gains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>levels comfortable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>gradual normalization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.091135</td>\n",
       "      <td>anti inflation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>easing process continue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>favorable evolution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>evolved expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>benign behavior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>anchored expectations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>prospects economic recovery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>consistent recovery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>favorable global economic activity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>gradual recovery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>consistent stabilization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>comfortable levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.636293</td>\n",
       "      <td>anti inflation program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.518250</td>\n",
       "      <td>auto workers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.391397</td>\n",
       "      <td>average closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.381318</td>\n",
       "      <td>average discount</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.233856</td>\n",
       "      <td>average discount rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.212195</td>\n",
       "      <td>average fell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.212134</td>\n",
       "      <td>average rose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.132480</td>\n",
       "      <td>average yield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.107583</td>\n",
       "      <td>bad news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.093391</td>\n",
       "      <td>balanced budget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.080926</td>\n",
       "      <td>bankers trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.054140</td>\n",
       "      <td>banking system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002646</td>\n",
       "      <td>basic money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.001691</td>\n",
       "      <td>bill rates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>-0.149100</td>\n",
       "      <td>wage price controls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-0.161947</td>\n",
       "      <td>wage settlements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>-0.200000</td>\n",
       "      <td>remains challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>-0.267513</td>\n",
       "      <td>weve seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-0.351935</td>\n",
       "      <td>wholesale prices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>-0.500000</td>\n",
       "      <td>supply shock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>-0.500000</td>\n",
       "      <td>economic recovery delayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>-0.500000</td>\n",
       "      <td>frustration expectations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-0.500000</td>\n",
       "      <td>growth slowdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>-0.500000</td>\n",
       "      <td>causing significant slowdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>-0.500000</td>\n",
       "      <td>uncertainty arises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>-0.500000</td>\n",
       "      <td>remains uncertain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-0.549839</td>\n",
       "      <td>work force</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>economic contraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>lower global growth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>historically low levels</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         coef                                word\n",
       "321  2.000000                conditions favorable\n",
       "309  2.000000            favorably expected signs\n",
       "317  2.000000                   evolved favorably\n",
       "318  2.000000                    benign evolution\n",
       "327  2.000000                   stronger recovery\n",
       "326  2.000000                  consumption growth\n",
       "324  2.000000                   recovery advances\n",
       "323  2.000000                    employment gains\n",
       "320  2.000000                  levels comfortable\n",
       "319  1.500000               gradual normalization\n",
       "0    1.091135                      anti inflation\n",
       "322  1.000000             easing process continue\n",
       "325  1.000000                 favorable evolution\n",
       "328  1.000000                    evolved expected\n",
       "330  1.000000                     benign behavior\n",
       "315  1.000000               anchored expectations\n",
       "314  1.000000         prospects economic recovery\n",
       "313  1.000000                 consistent recovery\n",
       "312  1.000000  favorable global economic activity\n",
       "311  1.000000                    gradual recovery\n",
       "310  1.000000            consistent stabilization\n",
       "331  1.000000                  comfortable levels\n",
       "1    0.636293              anti inflation program\n",
       "2    0.518250                        auto workers\n",
       "3    0.391397                      average closed\n",
       "4    0.381318                    average discount\n",
       "5    0.233856               average discount rate\n",
       "6    0.212195                        average fell\n",
       "7    0.212134                        average rose\n",
       "8    0.132480                       average yield\n",
       "9    0.107583                            bad news\n",
       "10   0.093391                     balanced budget\n",
       "11   0.080926                       bankers trust\n",
       "12   0.054140                      banking system\n",
       "13   0.002646                         basic money\n",
       "14   0.001691                          bill rates\n",
       "295 -0.149100                 wage price controls\n",
       "296 -0.161947                    wage settlements\n",
       "302 -0.200000                 remains challenging\n",
       "297 -0.267513                           weve seen\n",
       "298 -0.351935                    wholesale prices\n",
       "304 -0.500000                        supply shock\n",
       "305 -0.500000           economic recovery delayed\n",
       "316 -0.500000            frustration expectations\n",
       "300 -0.500000                     growth slowdown\n",
       "301 -0.500000        causing significant slowdown\n",
       "308 -0.500000                  uncertainty arises\n",
       "306 -0.500000                   remains uncertain\n",
       "299 -0.549839                          work force\n",
       "307 -1.000000                economic contraction\n",
       "329 -1.000000                 lower global growth\n",
       "303 -1.000000             historically low levels"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking LR coefs\n",
    "pd.DataFrame({\"coef\":pipeline_['clf'].coef_[0],\"word\":pipeline_['tfidf'].get_feature_names()}).sort_values(\"coef\", \n",
    "ascending=False).query(\"coef!=0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Name</th>\n",
       "      <th>Text</th>\n",
       "      <th>EconOutlook</th>\n",
       "      <th>Risks_Scenario</th>\n",
       "      <th>MonetaryPol</th>\n",
       "      <th>MonetPolDec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>copom 229 minutes</td>\n",
       "      <td>th update economic outlook global outlook nove...</td>\n",
       "      <td>global outlook novel coronavirus pandemic caus...</td>\n",
       "      <td>encompasses risk factors directions hand econo...</td>\n",
       "      <td>discussed evolution economic activity light av...</td>\n",
       "      <td>lower selic rate percentage judges decision re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>copom minutes 228</td>\n",
       "      <td>update economic outlook economic activity rele...</td>\n",
       "      <td>economic activity released previous indicate c...</td>\n",
       "      <td>encompasses risk factors directions hand high ...</td>\n",
       "      <td>discussed evolution economic activity light av...</td>\n",
       "      <td>lower selic rate percentage judges decision re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>copom minutes 230</td>\n",
       "      <td>update economic outlook scenario global outloo...</td>\n",
       "      <td>global outlook covid pandemic causing signific...</td>\n",
       "      <td>inflation encompasses risk factors directions ...</td>\n",
       "      <td>global economy judges previous international c...</td>\n",
       "      <td>lower selic rate percentage judges decision re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>copom_aug_5th</td>\n",
       "      <td>lowers selic rate lowers selic rate august pub...</td>\n",
       "      <td>following observations provide update global o...</td>\n",
       "      <td>prospectively uncertainty economic growth rema...</td>\n",
       "      <td>believes persevering process reforms necessary...</td>\n",
       "      <td>consequently possible future adjustments curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>min2017204th-copom20170117-204th_copom_minutes</td>\n",
       "      <td>thcopommeetinga update economic outlook scenar...</td>\n",
       "      <td>set indicators released recently suggests weak...</td>\n",
       "      <td>risks inflation short run inflation behavior f...</td>\n",
       "      <td>discussed evolution economic activity light re...</td>\n",
       "      <td>reduce selic rate year bias judges convergence...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            Name  \\\n",
       "0           0                               copom 229 minutes   \n",
       "1           1                               copom minutes 228   \n",
       "2           2                               copom minutes 230   \n",
       "3           3                                   copom_aug_5th   \n",
       "4           4  min2017204th-copom20170117-204th_copom_minutes   \n",
       "\n",
       "                                                Text  \\\n",
       "0  th update economic outlook global outlook nove...   \n",
       "1  update economic outlook economic activity rele...   \n",
       "2  update economic outlook scenario global outloo...   \n",
       "3  lowers selic rate lowers selic rate august pub...   \n",
       "4  thcopommeetinga update economic outlook scenar...   \n",
       "\n",
       "                                         EconOutlook  \\\n",
       "0  global outlook novel coronavirus pandemic caus...   \n",
       "1  economic activity released previous indicate c...   \n",
       "2  global outlook covid pandemic causing signific...   \n",
       "3  following observations provide update global o...   \n",
       "4  set indicators released recently suggests weak...   \n",
       "\n",
       "                                      Risks_Scenario  \\\n",
       "0  encompasses risk factors directions hand econo...   \n",
       "1  encompasses risk factors directions hand high ...   \n",
       "2  inflation encompasses risk factors directions ...   \n",
       "3  prospectively uncertainty economic growth rema...   \n",
       "4  risks inflation short run inflation behavior f...   \n",
       "\n",
       "                                         MonetaryPol  \\\n",
       "0  discussed evolution economic activity light av...   \n",
       "1  discussed evolution economic activity light av...   \n",
       "2  global economy judges previous international c...   \n",
       "3  believes persevering process reforms necessary...   \n",
       "4  discussed evolution economic activity light re...   \n",
       "\n",
       "                                         MonetPolDec  \n",
       "0  lower selic rate percentage judges decision re...  \n",
       "1  lower selic rate percentage judges decision re...  \n",
       "2  lower selic rate percentage judges decision re...  \n",
       "3  consequently possible future adjustments curre...  \n",
       "4  reduce selic rate year bias judges convergence...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing in BCB's texts\n",
    "#We are using a pre cleaned database with all the texts that matters for us - COPOM Minutes since 2016\n",
    "d2 = pd.read_excel('cleaned_data.xlsx')\n",
    "dv = pd.read_excel('base_val.xlsx')\n",
    "d2 = d2.dropna()\n",
    "d2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking how the algorithm predicts the texts for us\n",
    "#For the last minute, for example, we expect a negative sentiment, due to pandemic issues, etc\n",
    "pipeline_.predict(d2['MonetaryPol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61713158, 0.51781094, 0.52363385, 0.58325263, 0.52962619,\n",
       "       0.58325263, 0.58325263, 0.59992158, 0.52647961, 0.3946541 ,\n",
       "       0.20038519, 0.41402522, 0.52664025, 0.5714703 , 0.40125945,\n",
       "       0.52377556, 0.47825334, 0.52132955, 0.45342591, 0.52437899,\n",
       "       0.48182809, 0.56228385, 0.44275558, 0.54301707])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_.predict_proba(d2['MonetaryPol'])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_.predict(d2['Risks_Scenario'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
